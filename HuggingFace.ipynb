{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morten/mambaforge/envs/Hug/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoImageProcessor, MobileNetV2Model, AutoModelForSemanticSegmentation\n",
    "from datasets import load_dataset, Dataset, DatasetDict, Image\n",
    "PATH_DATASETS = '/home/morten/Documents/Algorithms-for-Automated-Driving/code/solutions/lane_detection/data_lane_segmentation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:01<00:00,  1.52s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:01<00:00,  1.88s/ba]88s/it]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [03:41<00:00, 221.28s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:02<00:00,  2.01s/ba].40s/it]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [03:50<00:00, 230.22s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:01<00:00,  1.86s/ba].62s/it]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [03:42<00:00, 222.26s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 4/4 [11:36<00:00, 174.11s/it]\n",
      "Pushing split validation to the Hub.\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:01<00:00,  1.44s/ba]\n",
      "Upload 1 LFS files: 100%|██████████| 1/1 [03:47<00:00, 227.16s/it]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [03:51<00:00, 231.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "img_dir = PATH_DATASETS + 'train/'\n",
    "label_dir = PATH_DATASETS + 'train_label/'\n",
    "\n",
    "image_path = []\n",
    "for dirname, _, filenames_fit in os.walk(img_dir):\n",
    "    for filename_fit in filenames_fit:\n",
    "        image_path.append(filename_fit.split('.')[0])\n",
    "\n",
    "label_path = []\n",
    "for dirname, _, filenames in os.walk(label_dir):\n",
    "    for filename in filenames:\n",
    "        label_path.append(filename.split('.')[0])\n",
    "\n",
    "img_fit = pd.DataFrame({'id': image_path}, index = np.arange(0, len(image_path)))\n",
    "label_fit = pd.DataFrame({'id': label_path}, index = np.arange(0, len(label_path)))\n",
    "\n",
    "lane_train_image_path, lane_val_image_path = train_test_split(img_fit['id'].values, test_size=0.2, train_size=0.8, random_state=10)\n",
    "\n",
    "lane_train_label_path = np.copy(lane_train_image_path)\n",
    "\n",
    "lane_val_label_path = np.copy(lane_val_image_path)\n",
    "\n",
    "\n",
    "for i in range(0, len(lane_train_image_path)):\n",
    "    lane_train_image_path[i] = img_dir + lane_train_image_path[i] + '.png'\n",
    "    lane_train_label_path[i] = label_dir + lane_train_label_path[i] + '_label.png'\n",
    "\n",
    "for i in range(0, len(lane_val_image_path)):\n",
    "    lane_val_image_path[i] = img_dir + lane_val_image_path[i] + '.png'\n",
    "    lane_val_label_path[i] = label_dir + lane_val_label_path[i] + '_label.png'\n",
    "\n",
    "def create_dataset(img_paths, lab_paths):\n",
    "    dataset = Dataset.from_dict({'image': img_paths,\n",
    "                                 'label': lab_paths})\n",
    "    dataset = dataset.cast_column('image', Image())\n",
    "    dataset = dataset.cast_column('label', Image())\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(lane_train_image_path, lane_train_label_path)\n",
    "val_dataset = create_dataset(lane_val_image_path, lane_val_label_path)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "})\n",
    "\n",
    "dataset.push_to_hub('lane_master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'background', 1: 'left', 2: 'right'}\n",
      "{0: 'background', 1: 'left', 2: 'right'}\n",
      "{'background': 0, 'left': 1, 'right': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/datasets/Efferbach/lane_master/blob/main/id2label.json'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "id2label = {0: 'background', 1: 'left', 2: 'right'}\n",
    "with open('id2label.json', 'w') as fp:\n",
    "    json.dump(id2label, fp)\n",
    "\n",
    "print(id2label)\n",
    "\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "print(id2label)\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(label2id)\n",
    "\n",
    "num_labels = len(id2label)\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"id2label.json\",\n",
    "    path_in_repo=\"id2label.json\",\n",
    "    repo_id=\"Efferbach/lane_master\",\n",
    "    repo_type=\"dataset\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a71f2f662bcb6f5889d2d37eee9ea51da62f6b07888de81c11f0a768a5b32622"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
